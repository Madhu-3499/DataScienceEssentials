{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhu-3499/DataScienceEssentials/blob/main/Madhu_surisetti_INFO5731_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 3**\n",
        "\n",
        "In this assignment, we will delve into various aspects of natural language processing (NLP) and text analysis. The tasks are designed to deepen your understanding of key NLP concepts and techniques, as well as to provide hands-on experience with practical applications.\n",
        "\n",
        "Through these tasks, you'll gain practical experience in NLP techniques such as N-gram analysis, TF-IDF, word embedding model creation, and sentiment analysis dataset creation.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: See Canvas\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "## Question 1 (30 points)\n",
        "\n",
        "**Understand N-gram**\n",
        "\n",
        "Write a python program to conduct N-gram analysis based on the dataset in your assignment two. You need to write codes from scratch instead of using any pre-existing libraries to do so:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9v8IikDpqrxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21962af-f219-4fdb-a393-52279d67db6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-gram: The worst one, Frequency: 1\n",
            "N-gram: worst one yet., Frequency: 1\n",
            "N-gram: They need to, Frequency: 1\n",
            "N-gram: need to stop, Frequency: 1\n",
            "N-gram: I literally thought, Frequency: 1\n",
            "N-gram: literally thought this, Frequency: 1\n",
            "N-gram: thought this was, Frequency: 1\n",
            "N-gram: this was a, Frequency: 1\n",
            "N-gram: was a parody, Frequency: 1\n",
            "N-gram: a parody skit, Frequency: 1\n",
            "N-gram: parody skit off, Frequency: 1\n",
            "N-gram: skit off of, Frequency: 1\n",
            "N-gram: off of SNL!, Frequency: 1\n",
            "N-gram: its comically bad, Frequency: 1\n",
            "N-gram: When \"Avengers\" meets, Frequency: 1\n",
            "N-gram: \"Avengers\" meets \"Fast, Frequency: 1\n",
            "N-gram: meets \"Fast and, Frequency: 1\n",
            "N-gram: \"Fast and the, Frequency: 1\n",
            "N-gram: and the furious\", Frequency: 1\n",
            "N-gram: the furious\" everything, Frequency: 1\n",
            "N-gram: furious\" everything goes, Frequency: 1\n",
            "N-gram: everything goes wrong, Frequency: 1\n",
            "N-gram: Toretto just started, Frequency: 1\n",
            "N-gram: just started playing, Frequency: 1\n",
            "N-gram: started playing Rocket, Frequency: 1\n",
            "N-gram: playing Rocket League, Frequency: 1\n",
            "N-gram: Rocket League in, Frequency: 1\n",
            "N-gram: League in the, Frequency: 1\n",
            "N-gram: in the middle, Frequency: 1\n",
            "N-gram: the middle of, Frequency: 1\n",
            "N-gram: middle of Italy., Frequency: 1\n",
            "N-gram: Void In between, Frequency: 1\n",
            "N-gram: In between car, Frequency: 1\n",
            "N-gram: between car action, Frequency: 1\n",
            "N-gram: Jason Momoa gives, Frequency: 1\n",
            "N-gram: Momoa gives one, Frequency: 1\n",
            "N-gram: gives one of, Frequency: 1\n",
            "N-gram: one of the, Frequency: 1\n",
            "N-gram: of the most, Frequency: 1\n",
            "N-gram: the most embarrassing, Frequency: 1\n",
            "N-gram: most embarrassing performances, Frequency: 1\n",
            "N-gram: embarrassing performances in, Frequency: 1\n",
            "N-gram: performances in film, Frequency: 1\n",
            "N-gram: in film history, Frequency: 1\n",
            "N-gram: Can't wait for, Frequency: 1\n",
            "N-gram: wait for the, Frequency: 1\n",
            "N-gram: for the franchise, Frequency: 1\n",
            "N-gram: the franchise to, Frequency: 1\n",
            "N-gram: franchise to be, Frequency: 1\n",
            "N-gram: to be over!, Frequency: 1\n",
            "N-gram: Absolute piece of, Frequency: 1\n",
            "N-gram: piece of crap, Frequency: 1\n",
            "N-gram: of crap that, Frequency: 1\n",
            "N-gram: crap that not, Frequency: 1\n",
            "N-gram: that not even, Frequency: 1\n",
            "N-gram: not even ends!!!, Frequency: 1\n",
            "N-gram: Weak fantasy with, Frequency: 1\n",
            "N-gram: fantasy with cars, Frequency: 1\n",
            "N-gram: Total disrespect for, Frequency: 1\n",
            "N-gram: disrespect for the, Frequency: 1\n",
            "N-gram: for the fans, Frequency: 1\n",
            "N-gram: The most entertaining, Frequency: 1\n",
            "N-gram: most entertaining part, Frequency: 1\n",
            "N-gram: entertaining part of, Frequency: 1\n",
            "N-gram: part of this, Frequency: 1\n",
            "N-gram: of this movie, Frequency: 1\n",
            "N-gram: this movie are, Frequency: 1\n",
            "N-gram: movie are the, Frequency: 1\n",
            "N-gram: are the ChatGPT, Frequency: 1\n",
            "N-gram: the ChatGPT 10/10, Frequency: 1\n",
            "N-gram: ChatGPT 10/10 reviews, Frequency: 1\n",
            "N-gram: The end of, Frequency: 1\n",
            "N-gram: end of the, Frequency: 1\n",
            "N-gram: of the road, Frequency: 1\n",
            "N-gram: the road begins, Frequency: 1\n",
            "N-gram: road begins badly, Frequency: 1\n",
            "N-gram: They couldn't even, Frequency: 1\n",
            "N-gram: couldn't even put, Frequency: 1\n",
            "N-gram: even put the, Frequency: 1\n",
            "N-gram: put the effort, Frequency: 1\n",
            "N-gram: the effort into, Frequency: 1\n",
            "N-gram: effort into a, Frequency: 1\n",
            "N-gram: into a better, Frequency: 1\n",
            "N-gram: a better title, Frequency: 1\n",
            "N-gram: Left me furious, Frequency: 1\n",
            "N-gram: What the heck, Frequency: 1\n",
            "N-gram: Even for F, Frequency: 1\n",
            "N-gram: for F +, Frequency: 1\n",
            "N-gram: F + F,, Frequency: 1\n",
            "N-gram: + F, it's, Frequency: 1\n",
            "N-gram: F, it's a, Frequency: 1\n",
            "N-gram: it's a Mess, Frequency: 1\n",
            "N-gram: Stupidity without coolness, Frequency: 1\n",
            "N-gram: This keeps getting, Frequency: 1\n",
            "N-gram: keeps getting worst, Frequency: 1\n",
            "N-gram: getting worst and, Frequency: 1\n",
            "N-gram: worst and worst., Frequency: 1\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "def read_cleaned_reviews_csv(file_path):\n",
        "    cleaned_reviews = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            cleaned_reviews.append(row[0])  # Assuming the first column contains cleaned reviews\n",
        "    return cleaned_reviews\n",
        "\n",
        "# Step 2: Count the frequency of all the N-grams (N=3)\n",
        "def count_ngrams(cleaned_reviews):\n",
        "    ngram_counts = defaultdict(int)\n",
        "    for review in cleaned_reviews:\n",
        "        ngrams = generate_ngrams(review, 3)\n",
        "        for ngram in ngrams:\n",
        "            ngram_counts[ngram] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    file_path = \"cleaned_reviews.csv\"  # Replace with your file path\n",
        "    cleaned_reviews = read_cleaned_reviews_csv(file_path)\n",
        "    ngram_counts = count_ngrams(cleaned_reviews)\n",
        "\n",
        "    # Output the frequency of all the N-grams\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        print(f\"N-gram: {ngram}, Frequency: {count}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "def read_cleaned_reviews_csv(file_path):\n",
        "    cleaned_reviews = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            cleaned_reviews.append(row[0])  # Assuming the first column contains cleaned reviews\n",
        "    return cleaned_reviews\n",
        "\n",
        "# Step 2: Calculate the probabilities for all the bigrams\n",
        "def calculate_bigram_probabilities(cleaned_reviews):\n",
        "    bigram_counts = defaultdict(int)\n",
        "    word_counts = defaultdict(int)\n",
        "\n",
        "    # Count bigrams and individual words\n",
        "    for review in cleaned_reviews:\n",
        "        words = review.split()\n",
        "        for i in range(len(words) - 1):\n",
        "            bigram = (words[i], words[i+1])\n",
        "            bigram_counts[bigram] += 1\n",
        "            word_counts[words[i]] += 1\n",
        "\n",
        "    # Calculate probabilities for each bigram\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        w2, w1 = bigram\n",
        "        probability = count / word_counts[w2]\n",
        "        bigram_probabilities[bigram] = probability\n",
        "\n",
        "    return bigram_probabilities\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    file_path = \"cleaned_reviews.csv\"  # Replace with your file path\n",
        "    cleaned_reviews = read_cleaned_reviews_csv(file_path)\n",
        "    bigram_probabilities = calculate_bigram_probabilities(cleaned_reviews)\n",
        "\n",
        "    # Output the probabilities for all the bigrams\n",
        "    for bigram, probability in bigram_probabilities.items():\n",
        "        print(f\"Bigram: {bigram}, Probability: {probability:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHRvcPAjyZTO",
        "outputId": "03527a59-c738-4416-8ffa-0d698a88043c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram: ('Excruciatingly', 'Awful'), Probability: 1.00\n",
            "Bigram: ('What', 'Happened?'), Probability: 0.50\n",
            "Bigram: ('The', 'worst'), Probability: 0.33\n",
            "Bigram: ('worst', 'one'), Probability: 0.50\n",
            "Bigram: ('one', 'yet.'), Probability: 0.50\n",
            "Bigram: ('Fast', 'X'), Probability: 1.00\n",
            "Bigram: ('They', 'need'), Probability: 0.50\n",
            "Bigram: ('need', 'to'), Probability: 1.00\n",
            "Bigram: ('to', 'stop'), Probability: 0.50\n",
            "Bigram: ('I', 'literally'), Probability: 1.00\n",
            "Bigram: ('literally', 'thought'), Probability: 1.00\n",
            "Bigram: ('thought', 'this'), Probability: 1.00\n",
            "Bigram: ('this', 'was'), Probability: 0.50\n",
            "Bigram: ('was', 'a'), Probability: 1.00\n",
            "Bigram: ('a', 'parody'), Probability: 0.33\n",
            "Bigram: ('parody', 'skit'), Probability: 1.00\n",
            "Bigram: ('skit', 'off'), Probability: 1.00\n",
            "Bigram: ('off', 'of'), Probability: 1.00\n",
            "Bigram: ('of', 'SNL!'), Probability: 0.17\n",
            "Bigram: ('its', 'comically'), Probability: 1.00\n",
            "Bigram: ('comically', 'bad'), Probability: 1.00\n",
            "Bigram: ('When', '\"Avengers\"'), Probability: 1.00\n",
            "Bigram: ('\"Avengers\"', 'meets'), Probability: 1.00\n",
            "Bigram: ('meets', '\"Fast'), Probability: 1.00\n",
            "Bigram: ('\"Fast', 'and'), Probability: 1.00\n",
            "Bigram: ('and', 'the'), Probability: 0.50\n",
            "Bigram: ('the', 'furious\"'), Probability: 0.11\n",
            "Bigram: ('furious\"', 'everything'), Probability: 1.00\n",
            "Bigram: ('everything', 'goes'), Probability: 1.00\n",
            "Bigram: ('goes', 'wrong'), Probability: 1.00\n",
            "Bigram: ('Toretto', 'just'), Probability: 1.00\n",
            "Bigram: ('just', 'started'), Probability: 1.00\n",
            "Bigram: ('started', 'playing'), Probability: 1.00\n",
            "Bigram: ('playing', 'Rocket'), Probability: 1.00\n",
            "Bigram: ('Rocket', 'League'), Probability: 1.00\n",
            "Bigram: ('League', 'in'), Probability: 1.00\n",
            "Bigram: ('in', 'the'), Probability: 0.50\n",
            "Bigram: ('the', 'middle'), Probability: 0.11\n",
            "Bigram: ('middle', 'of'), Probability: 1.00\n",
            "Bigram: ('of', 'Italy.'), Probability: 0.17\n",
            "Bigram: ('Void', 'In'), Probability: 1.00\n",
            "Bigram: ('In', 'between'), Probability: 1.00\n",
            "Bigram: ('between', 'car'), Probability: 1.00\n",
            "Bigram: ('car', 'action'), Probability: 1.00\n",
            "Bigram: ('Jason', 'Momoa'), Probability: 1.00\n",
            "Bigram: ('Momoa', 'gives'), Probability: 1.00\n",
            "Bigram: ('gives', 'one'), Probability: 1.00\n",
            "Bigram: ('one', 'of'), Probability: 0.50\n",
            "Bigram: ('of', 'the'), Probability: 0.33\n",
            "Bigram: ('the', 'most'), Probability: 0.11\n",
            "Bigram: ('most', 'embarrassing'), Probability: 0.50\n",
            "Bigram: ('embarrassing', 'performances'), Probability: 1.00\n",
            "Bigram: ('performances', 'in'), Probability: 1.00\n",
            "Bigram: ('in', 'film'), Probability: 0.50\n",
            "Bigram: ('film', 'history'), Probability: 1.00\n",
            "Bigram: ('Because...', 'Family'), Probability: 1.00\n",
            "Bigram: (\"Can't\", 'wait'), Probability: 1.00\n",
            "Bigram: ('wait', 'for'), Probability: 1.00\n",
            "Bigram: ('for', 'the'), Probability: 0.67\n",
            "Bigram: ('the', 'franchise'), Probability: 0.11\n",
            "Bigram: ('franchise', 'to'), Probability: 1.00\n",
            "Bigram: ('to', 'be'), Probability: 0.50\n",
            "Bigram: ('be', 'over!'), Probability: 1.00\n",
            "Bigram: ('Absolute', 'piece'), Probability: 1.00\n",
            "Bigram: ('piece', 'of'), Probability: 1.00\n",
            "Bigram: ('of', 'crap'), Probability: 0.17\n",
            "Bigram: ('crap', 'that'), Probability: 1.00\n",
            "Bigram: ('that', 'not'), Probability: 1.00\n",
            "Bigram: ('not', 'even'), Probability: 1.00\n",
            "Bigram: ('even', 'ends!!!'), Probability: 0.50\n",
            "Bigram: ('Weak', 'fantasy'), Probability: 1.00\n",
            "Bigram: ('fantasy', 'with'), Probability: 1.00\n",
            "Bigram: ('with', 'cars'), Probability: 1.00\n",
            "Bigram: ('Total', 'disrespect'), Probability: 1.00\n",
            "Bigram: ('disrespect', 'for'), Probability: 1.00\n",
            "Bigram: ('the', 'fans'), Probability: 0.11\n",
            "Bigram: ('The', 'most'), Probability: 0.33\n",
            "Bigram: ('most', 'entertaining'), Probability: 0.50\n",
            "Bigram: ('entertaining', 'part'), Probability: 1.00\n",
            "Bigram: ('part', 'of'), Probability: 1.00\n",
            "Bigram: ('of', 'this'), Probability: 0.17\n",
            "Bigram: ('this', 'movie'), Probability: 0.50\n",
            "Bigram: ('movie', 'are'), Probability: 1.00\n",
            "Bigram: ('are', 'the'), Probability: 1.00\n",
            "Bigram: ('the', 'ChatGPT'), Probability: 0.11\n",
            "Bigram: ('ChatGPT', '10/10'), Probability: 1.00\n",
            "Bigram: ('10/10', 'reviews'), Probability: 1.00\n",
            "Bigram: ('How', 'bad!'), Probability: 1.00\n",
            "Bigram: ('The', 'end'), Probability: 0.33\n",
            "Bigram: ('end', 'of'), Probability: 1.00\n",
            "Bigram: ('the', 'road'), Probability: 0.11\n",
            "Bigram: ('road', 'begins'), Probability: 1.00\n",
            "Bigram: ('begins', 'badly'), Probability: 1.00\n",
            "Bigram: ('They', \"couldn't\"), Probability: 0.50\n",
            "Bigram: (\"couldn't\", 'even'), Probability: 1.00\n",
            "Bigram: ('even', 'put'), Probability: 0.50\n",
            "Bigram: ('put', 'the'), Probability: 1.00\n",
            "Bigram: ('the', 'effort'), Probability: 0.11\n",
            "Bigram: ('effort', 'into'), Probability: 1.00\n",
            "Bigram: ('into', 'a'), Probability: 1.00\n",
            "Bigram: ('a', 'better'), Probability: 0.33\n",
            "Bigram: ('better', 'title'), Probability: 1.00\n",
            "Bigram: ('Left', 'me'), Probability: 1.00\n",
            "Bigram: ('me', 'furious'), Probability: 1.00\n",
            "Bigram: ('What', 'the'), Probability: 0.50\n",
            "Bigram: ('the', 'heck'), Probability: 0.11\n",
            "Bigram: ('Even', 'for'), Probability: 1.00\n",
            "Bigram: ('for', 'F'), Probability: 0.33\n",
            "Bigram: ('F', '+'), Probability: 1.00\n",
            "Bigram: ('+', 'F,'), Probability: 1.00\n",
            "Bigram: ('F,', \"it's\"), Probability: 1.00\n",
            "Bigram: (\"it's\", 'a'), Probability: 1.00\n",
            "Bigram: ('a', 'Mess'), Probability: 0.33\n",
            "Bigram: ('Stupidity', 'without'), Probability: 1.00\n",
            "Bigram: ('without', 'coolness'), Probability: 1.00\n",
            "Bigram: ('This', 'keeps'), Probability: 1.00\n",
            "Bigram: ('keeps', 'getting'), Probability: 1.00\n",
            "Bigram: ('getting', 'worst'), Probability: 1.00\n",
            "Bigram: ('worst', 'and'), Probability: 0.50\n",
            "Bigram: ('and', 'worst.'), Probability: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "def read_cleaned_reviews_csv(file_path):\n",
        "    cleaned_reviews = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            cleaned_reviews.append(row[0])  # Assuming the first column contains cleaned reviews\n",
        "    return cleaned_reviews\n",
        "\n",
        "# Step 2: Extract noun phrases from each review\n",
        "def extract_noun_phrases(review):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(review)\n",
        "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "    return noun_phrases\n",
        "\n",
        "# Step 3: Count the frequency of each noun phrase across all reviews\n",
        "def count_noun_phrases(cleaned_reviews):\n",
        "    noun_phrase_frequencies = defaultdict(int)\n",
        "    for review in cleaned_reviews:\n",
        "        noun_phrases = extract_noun_phrases(review)\n",
        "        for noun_phrase in noun_phrases:\n",
        "            noun_phrase_frequencies[noun_phrase] += 1\n",
        "    return noun_phrase_frequencies\n",
        "\n",
        "# Step 4: Calculate relative probabilities of each review in terms of other reviews\n",
        "def calculate_relative_probabilities(cleaned_reviews, noun_phrase_frequencies):\n",
        "    max_frequency = max(noun_phrase_frequencies.values())\n",
        "    relative_probabilities = defaultdict(dict)\n",
        "    for i, review in enumerate(cleaned_reviews):\n",
        "        noun_phrases = extract_noun_phrases(review)\n",
        "        for noun_phrase in noun_phrases:\n",
        "            frequency = noun_phrase_frequencies[noun_phrase]\n",
        "            relative_probabilities[i][noun_phrase] = frequency / max_frequency\n",
        "    return relative_probabilities\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    file_path = \"cleaned_reviews.csv\"  # Replace with your file path\n",
        "    cleaned_reviews = read_cleaned_reviews_csv(file_path)\n",
        "    noun_phrase_frequencies = count_noun_phrases(cleaned_reviews)\n",
        "    relative_probabilities = calculate_relative_probabilities(cleaned_reviews, noun_phrase_frequencies)\n",
        "\n",
        "    # Print the results in a table format\n",
        "    noun_phrases = set(noun_phrase for frequencies in relative_probabilities.values() for noun_phrase in frequencies.keys())\n",
        "\n",
        "    print(\"{:<10}\".format(\"\"), end=\"\")  # Print header row\n",
        "    for noun_phrase in noun_phrases:\n",
        "        print(\"{:<20}\".format(noun_phrase), end=\"\")\n",
        "    print()\n",
        "\n",
        "    for i, review_probabilities in relative_probabilities.items():\n",
        "        print(\"{:<10}\".format(f\"Review {i+1}\"), end=\"\")\n",
        "        for noun_phrase in noun_phrases:\n",
        "            print(\"{:<20}\".format(review_probabilities.get(noun_phrase, 0)), end=\"\")\n",
        "        print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP2spg8Uyo8_",
        "outputId": "9564f20a-34b8-4feb-84ad-a06db3cec2ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          \"Avengers           Italy               The end             the fans            me                  Weak fantasy        This                Jason Momoa         Fast X              the middle          film history        cars                a Mess              car action          The most entertaining partI                   Toretto             Void                What                What the heck       Rocket League       coolness            They                Stupidity           a better title      review_title        Total disrespect    the road            this                the franchise       the ChatGPT 10/10 reviewsthis movie          a parody skit       Family              it                  the effort          crap                that                Excruciatingly AwfulAbsolute piece      the furious\" everythingSNL                 the most embarrassing performancesF + F               \n",
            "Review 1  0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 2  0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   \n",
            "Review 3  0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 5  0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 6  0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   1.0                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 7  0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   \n",
            "Review 9  0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   \n",
            "Review 10 0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 11 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 12 0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   \n",
            "Review 13 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 14 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 15 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0.5                 0                   0.5                 0                   0                   0                   0                   \n",
            "Review 16 0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 17 0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 18 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 20 0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 21 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   1.0                 0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 22 0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 23 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 24 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0.5                 \n",
            "Review 25 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0.5                 0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n",
            "Review 26 0                   0                   0                   0                   0                   0                   0.5                 0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   0                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "## Question 2 (25 points)\n",
        "\n",
        "**Undersand TF-IDF and Document representation**\n",
        "\n",
        "Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the documents-terms weights (tf * idf) matrix.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using cosine similarity.\n",
        "\n",
        "Note: You need to write codes from scratch instead of using any pre-existing libraries to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LjN0iysvo9-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63cb7ec-2729-4c25-84c8-350d46ab40f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: can't wait for the franchise to be over!\n",
            "\n",
            "Ranked Documents:\n",
            "Document 14: Similarity = 1.0000\n",
            "Document 6: Similarity = 0.1521\n",
            "Document 17: Similarity = 0.1340\n",
            "Document 24: Similarity = 0.0721\n",
            "Document 20: Similarity = 0.0411\n",
            "Document 23: Similarity = 0.0331\n",
            "Document 18: Similarity = 0.0298\n",
            "Document 4: Similarity = 0.0281\n",
            "Document 21: Similarity = 0.0159\n",
            "Document 9: Similarity = 0.0150\n",
            "Document 10: Similarity = 0.0150\n",
            "Document 12: Similarity = 0.0146\n",
            "Document 1: Similarity = 0.0000\n",
            "Document 2: Similarity = 0.0000\n",
            "Document 3: Similarity = 0.0000\n",
            "Document 5: Similarity = 0.0000\n",
            "Document 7: Similarity = 0.0000\n",
            "Document 8: Similarity = 0.0000\n",
            "Document 11: Similarity = 0.0000\n",
            "Document 13: Similarity = 0.0000\n",
            "Document 15: Similarity = 0.0000\n",
            "Document 16: Similarity = 0.0000\n",
            "Document 19: Similarity = 0.0000\n",
            "Document 22: Similarity = 0.0000\n",
            "Document 25: Similarity = 0.0000\n",
            "Document 26: Similarity = 0.0000\n",
            "\n",
            "TF-IDF Matrix:\n",
            "Document 1: {'review_title': 2.5649493574615367}\n",
            "Document 2: {'excruciatingly': 1.2824746787307684, 'awful': 1.2824746787307684}\n",
            "Document 3: {'what': 1.079742124676686, 'happened?': 1.2824746787307684}\n",
            "Document 4: {'the': 0.2150503163057779, 'worst': 0.539871062338343, 'one': 0.539871062338343, 'yet': 0.6412373393653842}\n",
            "Document 5: {'fast': 1.2824746787307684, 'x': 1.2824746787307684}\n",
            "Document 6: {'they': 0.539871062338343, 'need': 0.6412373393653842, 'to': 0.539871062338343, 'stop': 0.6412373393653842}\n",
            "Document 7: {'i': 0.23317721431468516, 'literally': 0.23317721431468516, 'thought': 0.23317721431468516, 'this': 0.17016383426378104, 'was': 0.23317721431468516, 'a': 0.17016383426378104, 'parody': 0.23317721431468516, 'skit': 0.23317721431468516, 'off': 0.23317721431468516, 'of': 0.11928967172419716, 'snl!': 0.23317721431468516}\n",
            "Document 8: {'its': 0.8549831191538455, 'comically': 0.8549831191538455, 'bad': 0.8549831191538455}\n",
            "Document 9: {'when': 0.2564949357461537, '\"avengers\"': 0.2564949357461537, 'meets': 0.2564949357461537, '\"fast': 0.2564949357461537, 'and': 0.21594842493533722, 'the': 0.08602012652231117, 'furious\"': 0.2564949357461537, 'everything': 0.2564949357461537, 'goes': 0.2564949357461537, 'wrong': 0.2564949357461537}\n",
            "Document 10: {'toretto': 0.23317721431468516, 'just': 0.23317721431468516, 'started': 0.23317721431468516, 'playing': 0.23317721431468516, 'rocket': 0.23317721431468516, 'league': 0.23317721431468516, 'in': 0.17016383426378104, 'the': 0.07820011502028287, 'middle': 0.23317721431468516, 'of': 0.11928967172419716, 'italy': 0.23317721431468516}\n",
            "Document 11: {'void': 0.5129898714923073, 'in': 0.37436043538031827, 'between': 0.5129898714923073, 'car': 0.5129898714923073, 'action': 0.5129898714923073}\n",
            "Document 12: {'jason': 0.21374577978846138, 'momoa': 0.21374577978846138, 'gives': 0.21374577978846138, 'one': 0.17995702077944767, 'of': 0.10934886574718072, 'the': 0.07168343876859262, 'most': 0.17995702077944767, 'embarrassing': 0.21374577978846138, 'performances': 0.21374577978846138, 'in': 0.15598351474179928, 'film': 0.21374577978846138, 'history': 0.21374577978846138}\n",
            "Document 13: {'because': 1.2824746787307684, 'family': 1.2824746787307684}\n",
            "Document 14: {\"can't\": 0.3206186696826921, 'wait': 0.3206186696826921, 'for': 0.23397527211269892, 'the': 0.10752515815288895, 'franchise': 0.3206186696826921, 'to': 0.2699355311691715, 'be': 0.3206186696826921, 'over!': 0.3206186696826921}\n",
            "Document 15: {'absolute': 0.3206186696826921, 'piece': 0.3206186696826921, 'of': 0.16402329862077109, 'crap': 0.3206186696826921, 'that': 0.3206186696826921, 'not': 0.3206186696826921, 'even': 0.23397527211269892, 'ends!!!': 0.3206186696826921}\n",
            "Document 16: {'weak': 0.6412373393653842, 'fantasy': 0.6412373393653842, 'with': 0.6412373393653842, 'cars': 0.6412373393653842}\n",
            "Document 17: {'total': 0.5129898714923073, 'disrespect': 0.5129898714923073, 'for': 0.37436043538031827, 'the': 0.17204025304462234, 'fans': 0.5129898714923073}\n",
            "Document 18: {'the': 0.14336687753718524, 'most': 0.17995702077944767, 'entertaining': 0.21374577978846138, 'part': 0.21374577978846138, 'of': 0.10934886574718072, 'this': 0.15598351474179928, 'movie': 0.21374577978846138, 'are': 0.21374577978846138, 'chatgpt': 0.21374577978846138, '10/10': 0.21374577978846138, 'reviews': 0.21374577978846138}\n",
            "Document 19: {'how': 1.2824746787307684, 'bad!': 1.2824746787307684}\n",
            "Document 20: {'the': 0.24577179006374616, 'end': 0.3664213367802195, 'of': 0.18745519842373837, 'road': 0.3664213367802195, 'begins': 0.3664213367802195, 'badly': 0.3664213367802195}\n",
            "Document 21: {'they': 0.21594842493533722, \"couldn't\": 0.2564949357461537, 'even': 0.18718021769015913, 'put': 0.2564949357461537, 'the': 0.08602012652231117, 'effort': 0.2564949357461537, 'into': 0.2564949357461537, 'a': 0.18718021769015913, 'better': 0.2564949357461537, 'title': 0.2564949357461537}\n",
            "Document 22: {'left': 0.8549831191538455, 'me': 0.8549831191538455, 'furious': 0.8549831191538455}\n",
            "Document 23: {'what': 0.7198280831177907, 'the': 0.2867337550743705, 'heck': 0.8549831191538455}\n",
            "Document 24: {'even': 0.23397527211269892, 'for': 0.23397527211269892, 'f': 0.6412373393653842, '+': 0.3206186696826921, \"it's\": 0.3206186696826921, 'a': 0.23397527211269892, 'mess': 0.3206186696826921}\n",
            "Document 25: {'stupidity': 0.8549831191538455, 'without': 0.8549831191538455, 'coolness': 0.8549831191538455}\n",
            "Document 26: {'this': 0.31196702948359856, 'keeps': 0.42749155957692275, 'getting': 0.42749155957692275, 'worst': 0.7198280831177907, 'and': 0.35991404155889534}\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Step 1: Preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = text.lower().replace('.', '').replace(',', '').split()\n",
        "    return text\n",
        "\n",
        "# Step 2: Tokenize text\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Step 3: Calculate Term Frequency (TF)\n",
        "def calculate_tf(text):\n",
        "    tf = {}\n",
        "    total_words = len(text)\n",
        "    for word in text:\n",
        "        tf[word] = tf.get(word, 0) + 1 / total_words\n",
        "    return tf\n",
        "\n",
        "# Step 4: Calculate Inverse Document Frequency (IDF)\n",
        "def calculate_idf(documents):\n",
        "    idf = {}\n",
        "    total_documents = len(documents)\n",
        "\n",
        "    # Count the number of documents containing each term\n",
        "    for document in documents:\n",
        "        unique_words = set(document)\n",
        "        for word in unique_words:\n",
        "            idf[word] = idf.get(word, 0) + 1\n",
        "\n",
        "    # Calculate IDF\n",
        "    for word, freq in idf.items():\n",
        "        idf[word] = math.log(total_documents / (freq + 1))  # Adding 1 to avoid division by zero\n",
        "\n",
        "    return idf\n",
        "\n",
        "# Step 5: Build TF-IDF matrix\n",
        "def build_tf_idf_matrix(documents, idf):\n",
        "    tf_idf_matrix = []\n",
        "    for document in documents:\n",
        "        tf = calculate_tf(document)\n",
        "        tf_idf_vector = {word: tf_value * idf[word] for word, tf_value in tf.items()}\n",
        "        tf_idf_matrix.append(tf_idf_vector)\n",
        "    return tf_idf_matrix\n",
        "\n",
        "# Step 6: Calculate cosine similarity\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = sum(vector1.get(term, 0) * vector2.get(term, 0) for term in set(vector1) | set(vector2))\n",
        "    magnitude1 = math.sqrt(sum(val ** 2 for val in vector1.values()))\n",
        "    magnitude2 = math.sqrt(sum(val ** 2 for val in vector2.values()))\n",
        "    if magnitude1 == 0 or magnitude2 == 0:\n",
        "        return 0\n",
        "    return dot_product / (magnitude1 * magnitude2)\n",
        "\n",
        "# Step 7: Rank documents based on cosine similarity\n",
        "def rank_documents(query_vector, tf_idf_matrix):\n",
        "    rankings = []\n",
        "    for i, document_vector in enumerate(tf_idf_matrix):\n",
        "        similarity = cosine_similarity(query_vector, document_vector)\n",
        "        rankings.append((i, similarity))\n",
        "    rankings.sort(key=lambda x: x[1], reverse=True)\n",
        "    return rankings\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    file_path = \"cleaned_reviews.csv\"  # Replace with your file path\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        cleaned_reviews = [row[0] for row in reader]\n",
        "\n",
        "    # Randomly select a query from the cleaned reviews\n",
        "    query = random.choice(cleaned_reviews)\n",
        "\n",
        "    # Preprocess documents and query\n",
        "    preprocessed_documents = [preprocess_text(doc) for doc in cleaned_reviews]\n",
        "    preprocessed_query = preprocess_text(query)\n",
        "\n",
        "    # Calculate IDF\n",
        "    idf = calculate_idf(preprocessed_documents)\n",
        "\n",
        "    # Build TF-IDF matrix for documents and query\n",
        "    tf_idf_matrix = build_tf_idf_matrix(preprocessed_documents, idf)\n",
        "    query_tf_idf = {term: tf_value * idf[term] for term, tf_value in calculate_tf(preprocessed_query).items()}\n",
        "\n",
        "    # Rank documents based on cosine similarity with the query\n",
        "    rankings = rank_documents(query_tf_idf, tf_idf_matrix)\n",
        "\n",
        "    # Print the query\n",
        "    print(\"Query:\", ' '.join(preprocessed_query))\n",
        "\n",
        "    # Print ranked documents\n",
        "    print(\"\\nRanked Documents:\")\n",
        "    for i, similarity in rankings:\n",
        "        print(f\"Document {i + 1}: Similarity = {similarity:.4f}\")\n",
        "\n",
        "    # Print TF-IDF matrix\n",
        "    print(\"\\nTF-IDF Matrix:\")\n",
        "    for i, doc_tf_idf in enumerate(tf_idf_matrix):\n",
        "        print(f\"Document {i + 1}: {doc_tf_idf}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "## Question 3 (25 points)\n",
        "\n",
        "**Create your own word embedding model**\n",
        "\n",
        "Use the data you collected for assignment 2 to build a word embedding model:\n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
        "\n",
        "(2) Visualize the word embedding model you created.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eczZgyAoo05Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "ab5b5fed-8c17-461a-8350-bb12d6bc82c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install or upgrade Gensim library\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "# Importing necessary libraries\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "\n",
        "# Step 1: Read sentences from CSV file\n",
        "def read_sentences_from_csv(file_path):\n",
        "    sentences = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            sentences.append(row[0].split())  # Assuming the first column contains sentences\n",
        "    return sentences\n",
        "\n",
        "# Step 2: Train Word2Vec model\n",
        "def train_word2vec_model(sentences):\n",
        "    # Training Word2Vec model with a 300-dimensional vector size\n",
        "    word2vec_model = Word2Vec(sentences, vector_size=300)\n",
        "    return word2vec_model\n",
        "\n",
        "# Step 3: Visualize word embeddings using PCA\n",
        "def visualize_embeddings(model):\n",
        "    vocab = list(model.wv.index_to_key)\n",
        "    X = model.wv[vocab]\n",
        "    pca = PCA(n_components=2)\n",
        "    result = pca.fit_transform(X)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(result[:, 0], result[:, 1])\n",
        "    for i, word in enumerate(vocab):\n",
        "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    file_path = \"cleaned_reviews.csv\"  # Replace with your file path\n",
        "    sentences = read_sentences_from_csv(file_path)\n",
        "    model = train_word2vec_model(sentences)\n",
        "    visualize_embeddings(model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoVp3aYoU8F"
      },
      "source": [
        "## Question 4 (20 Points)\n",
        "\n",
        "**Create your own training and evaluation data for sentiment analysis.**\n",
        "\n",
        " **You don't need to write program for this question!**\n",
        "\n",
        " For example, if you collected a movie review or a product review data, then you can do the following steps:\n",
        "\n",
        "*   Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral).\n",
        "\n",
        "*   Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew.\n",
        "\n",
        "*   This datset will be used for assignment four: sentiment analysis and text classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DyK54UY6ompS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Link: https://github.com/Madhu-3499/DataScienceEssentials/blob/main/annotated_reviews.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The assignment covers a wide range of natural language processing tasks, including data preprocessing, N-gram analysis, TF-IDF calculation, sentiment analysis, word embedding training, and visualization. While each task presents its own challenges, such as data cleaning and implementing algorithms from scratch, the diversity of tasks allows for a comprehensive exploration of natural language processing concepts. Despite the potential complexity of training word embeddings from scratch, the hands-on experience and creative freedom provided by the assignment make it an enjoyable learning opportunity. The time required to complete the assignment may vary depending on prior experience and dataset complexity, but with dedication and effort, completing the tasks within a reasonable timeframe is achievable."
      ],
      "metadata": {
        "id": "2aMbJvQk2NZ2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}